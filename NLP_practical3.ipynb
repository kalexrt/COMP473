{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding vectors and matrices"
      ],
      "metadata": {
        "id": "WGQG9oLVfamF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFEjKGsHa9sp",
        "outputId": "ecc7d799-7d43-4d93-e6a3-21a98ab36142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n",
            "[[0 1 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 1]\n",
            " [1 0 1 1 1 0 1 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "x= (\"Computers can analyze text\",\"They do it using vectors and matrices\",\"Computers can process massive amounts of text data\")\n",
        "vectorizer= CountVectorizer(stop_words='english')\n",
        "x_vec= vectorizer.fit_transform(x)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(x_vec.todense())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the bag of words architecture"
      ],
      "metadata": {
        "id": "166Zy-vzfgoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uyUOC02fjO1",
        "outputId": "c3e1c82f-cc70-4e32-c5f3-82c67a3bc537"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "\"Natural Language Processing making computers comprehend language data\",\n",
        "\"The field of Natural Language Processing is evolving everyday\"]"
      ],
      "metadata": {
        "id": "V7Dlszoufm_j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus= pd.Series(sentences)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZQ22r2Mfqj5",
        "outputId": "c6c9814c-fd66-4a78-b704-e0ffc8bb51b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "ozUrzBqiftcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "x_vQIjTqf1A0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "jMMv4T5Xf4Dc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "1ZuLLkxlf7vx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "wsQmSNYFf93b"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "Af7ulKBsgA7c"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"
      ],
      "metadata": {
        "id": "aPLVlbk6gBwd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing with Lemmatization here"
      ],
      "metadata": {
        "id": "-cvaHxqAgJsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFpBEAUWgKkD",
        "outputId": "abe42939-4fc9-447f-f3b5-8067bda0824e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f8a15d8180b8>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-5-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-5-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-5-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_of_words = set()\n",
        "for sentence in preprocessed_corpus:\n",
        "    for word in sentence.split():\n",
        "        set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgBUMPUlgRHi",
        "outputId": "8b3403e6-4697-4057-b646-4717edc93f16"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['everyday', 'data', 'comprehend', 'computers', 'language', 'read', 'field', 'evolve', 'natural', 'make', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPOBiGPlgVjH",
        "outputId": "35d0e4e9-c598-4cdf-9453-a62e72bf42f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'everyday': 0, 'data': 1, 'comprehend': 2, 'computers': 3, 'language': 4, 'read': 5, 'field': 6, 'evolve': 7, 'natural': 8, 'make': 9, 'process': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a matrix to hold the Bag of Words representation"
      ],
      "metadata": {
        "id": "_8H9a5HkgMF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
      ],
      "metadata": {
        "id": "H-LZ_uelgXcI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
        "    for token in preprocessed_sentence.split():\n",
        "        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"
      ],
      "metadata": {
        "id": "3IiRCKvygaVW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbi_60LMgb_f",
        "outputId": "e303c9c5-c31b-4870-d48d-ecec5088c8b0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1.],\n",
              "       [0., 1., 1., 1., 2., 0., 0., 0., 1., 1., 1.],\n",
              "       [1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizer"
      ],
      "metadata": {
        "id": "BQFEMH_-gegw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "3BV8rr8EgctI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHYcefkNgi5X",
        "outputId": "986740f8-604c-4409-fd54-1908e4a5a448"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(bow_matrix.toarray().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcXrdcnzglVA",
        "outputId": "f6cfae3f-a654-4cad-cd30-7f49c0bf7f42"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
        "bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "NLIode3HgnDM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer_ngram_range.get_feature_names_out())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC014o4qgn2v",
        "outputId": "4616a6e3-3d3e-4fb6-dd52-bb44c04296b5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'comprehend language' 'comprehend language data' 'computers'\n",
            " 'computers comprehend' 'computers comprehend language' 'data' 'everyday'\n",
            " 'evolve' 'evolve everyday' 'field' 'field natural'\n",
            " 'field natural language' 'language' 'language data' 'language process'\n",
            " 'language process evolve' 'language process make' 'make' 'make computers'\n",
            " 'make computers comprehend' 'natural' 'natural language'\n",
            " 'natural language process' 'process' 'process evolve'\n",
            " 'process evolve everyday' 'process make' 'process make computers' 'read'\n",
            " 'read natural' 'read natural language']\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3),\n",
        "max_features = 6)\n",
        "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_max_features.get_feature_names_out())\n",
        "print(bow_matrix_max_features.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTsVyseRgqdE",
        "outputId": "32635443-e001-4076-db39-c4a479d3b6f5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_df= 3, min_df = 2)\n",
        "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_max_features.get_feature_names_out())\n",
        "print(bow_matrix_max_features.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ax_n5b2gr9K",
        "outputId": "81cb9746-96be-4d18-940b-f02fb7571959"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF vectors"
      ],
      "metadata": {
        "id": "IkgChAyugxei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "BOmO0FpigwQn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VT-bipfg3f4",
        "outputId": "d3da9e50-64e6-45f6-f231-23b393d93ceb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n",
        "tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "qgLLH3lVg4_9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer_l1_norm.get_feature_names_out())\n",
        "print(tf_idf_matrix_l1_norm.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZYJuT12g6bK",
        "outputId": "40b22e01-0231-41a0-96ab-0f97a93cf934"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n",
            " [0.1571718  0.1571718  0.1571718  0.         0.         0.\n",
            "  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n",
            " [0.         0.         0.         0.2095624  0.2095624  0.2095624\n",
            "  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the norm to L1"
      ],
      "metadata": {
        "id": "q9b3psFYhA22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n",
        "tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_l1_norm.get_feature_names_out())\n",
        "print(tf_idf_matrix_l1_norm.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYCa8YljhBrh",
        "outputId": "449e5192-9597-4bd8-810d-553191383a93"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n",
            " [0.1571718  0.1571718  0.1571718  0.         0.         0.\n",
            "  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n",
            " [0.         0.         0.         0.2095624  0.2095624  0.2095624\n",
            "  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams and Max features with TfidfVectorizer"
      ],
      "metadata": {
        "id": "ImqGAHRohF14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\", analyzer='word', ngram_range=(1,3), max_features=6)\n",
        "tf_idf_matrix_n_gram_max_features = vectorizer_n_gram_max_features.fit_transform(preprocessed_corpus)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = vectorizer_n_gram_max_features.get_feature_names_out()\n",
        "\n",
        "# Print the top feature names\n",
        "print(\"Top feature names:\", feature_names[:6])\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tf_idf_matrix_n_gram_max_features.toarray())\n",
        "\n",
        "# Print the shape of the TF-IDF matrix\n",
        "print(\"\\nThe shape of the TF-IDF matrix is:\", tf_idf_matrix_n_gram_max_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i6yopQShDQx",
        "outputId": "36b4ce8e-3129-413e-93c3-3a46094e2516"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top feature names: ['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
            " [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n",
            "\n",
            "The shape of the TF-IDF matrix is: (3, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance/similarity calculation between document vectors"
      ],
      "metadata": {
        "id": "S3-TtdXThMD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine Similarity Calculation"
      ],
      "metadata": {
        "id": "1Vw-az7MhPLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"
      ],
      "metadata": {
        "id": "5wNj1ZUOhI_6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CountVectorizer"
      ],
      "metadata": {
        "id": "mWvFuN__hSFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "PNXF1PpfhT81"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMRqxDVrhVcM",
        "outputId": "db2643cc-a456-4f34-9a0d-7865b8f9903f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine similarity between the document vectors built using CountVectorizer"
      ],
      "metadata": {
        "id": "x6b407-_hXVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(bow_matrix.shape[0]):\n",
        "    for j in range(i + 1, bow_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUkrk_okhbIg",
        "outputId": "f1dfc9d9-13c3-443a-bc7a-5993014f9471"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n",
            "The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n",
            "The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "EEk4z5iPhcwz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKKp9NtMhgVn",
        "outputId": "159a01dc-2ec2-43a0-a79c-08ba33fc78b1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine similarity between the document vectors built using TfidfVectorizer"
      ],
      "metadata": {
        "id": "3pQwNkT3hjp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(tf_idf_matrix.shape[0]):\n",
        "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aisA_SnhkTm",
        "outputId": "da4b7fd0-3326-4801-af5f-af292b45c724"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n",
            "The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n",
            "The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot vectorization"
      ],
      "metadata": {
        "id": "YusRuYyphmG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = [\"We are reading about Natural Language Processing Here\"]\n",
        "corpus = pd.Series(sentence)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYCFM97phoaE",
        "outputId": "fe4fae03-81b3-4171-89a7-35a91d3ee4e7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming =\n",
        "False, stem_type = None,lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbgUOgy9hqji",
        "outputId": "cca08209-7516-4763-9c5f-93af5890d44b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f8a15d8180b8>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-5-f8a15d8180b8>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['read natural language process']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building the vocabulary\n",
        "set_of_words = set()\n",
        "for word in preprocessed_corpus[0].split():\n",
        "    set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvTWEtQDhuRr",
        "outputId": "06b1965e-7bb4-4957-cf44-710e1600b039"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'read', 'language', 'process']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetching the position of each word in the vocabulary\n",
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMTu93Q3hyEN",
        "outputId": "4c34449b-1c7e-44ed-d05f-e8f58a2b7582"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'natural': 0, 'read': 1, 'language': 2, 'process': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiating the one hot matrix\n",
        "\n",
        "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n",
        "one_hot_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OA48Uq4h2T_",
        "outputId": "3caa7eb8-d508-421c-a494-f505aaca8949"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Building One Hot Vectors\n",
        "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
        "    one_hot_matrix[i][position[token]] = 1"
      ],
      "metadata": {
        "id": "IyEhjXi3h7sz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUUo7GxWh_xy",
        "outputId": "1940b01d-1650-4d15-b488-b09e5b83ab1d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a basic chatbot"
      ],
      "metadata": {
        "id": "9lqus0BHiCaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import chardet"
      ],
      "metadata": {
        "id": "iddk_N-NiFAy"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading questions and answers in separate lists\n",
        "import ast\n",
        "questions = []\n",
        "answers = []\n",
        "# with open('/content/qa_Electronics.json.gz', 'rb') as f:\n",
        "#     encoding = chardet.detect(f.read())['encoding']\n",
        "with open('/content/qa_Electronics.json','r') as f:\n",
        "    for line in f:\n",
        "        data = ast.literal_eval(line)\n",
        "        questions.append(data['question'].lower())\n",
        "        answers.append(data['answer'].lower())"
      ],
      "metadata": {
        "id": "s3aEwqCYiYdA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text and convert data in matrix format\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(questions)"
      ],
      "metadata": {
        "id": "YcyPBkUhiaPu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform data by applying term frequency inverse document frequency (TF-IDF)\n",
        "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
        "X_tfidf = tfidf.fit_transform(X_vec)"
      ],
      "metadata": {
        "id": "ZyvtutjkkEjg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conversation(im):\n",
        "    global tfidf, answers, X_tfidf\n",
        "    Y_vec = vectorizer.transform(im)\n",
        "    Y_tfidf = tfidf.fit_transform(Y_vec)\n",
        "    cos_sim = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n",
        "    if cos_sim > 60 :\n",
        "        return \"sorry, I did not quite understand that\"\n",
        "    else:\n",
        "        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]"
      ],
      "metadata": {
        "id": "tklM_ALkkGJC"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    usr = input(\"Please enter your username: \")\n",
        "    print(\"support: Hi, welcome to Q&A support. How can I help you?\")\n",
        "    while True:\n",
        "        im = input(\"{}: \".format(usr))\n",
        "        if im.lower() == 'bye':\n",
        "            print(\"Q&A support: bye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Q&A support: \"+conversation([im]))\n"
      ],
      "metadata": {
        "id": "vHlvvQAxkHcQ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "bOi26QWakKcr",
        "outputId": "8f515786-ba3b-4ca5-dcfc-6f90293d0251"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your username: kalash\n",
            "support: Hi, welcome to Q&A support. How can I help you?\n",
            "kalash: what can i buy\n",
            "Q&A support: walgreens usually carries these for $9.99 on sale, $11.99 regular price. i just checked two weeks ago they had a couple of pairs in stock. i bought my last ones at target for $10.00. this seller is smoking crack! $94.99 would be a decent price for a pack of 10 if you could forgo shipping costs. check walgreens.\n",
            "kalash: when does the sale end\n",
            "Q&A support: it's a 1-year warranty from date of purchase.\n",
            "kalash: what?\n",
            "Q&A support: sorry, I did not quite understand that\n",
            "kalash: is there a discount\n",
            "Q&A support: ok, 23.95$x3,can it? thanks\n",
            "kalash: thank you\n",
            "Q&A support: hi. frame china, optics swiss\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-d704e3800569>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"support: Hi, welcome to Q&A support. How can I help you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'bye'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Q&A support: bye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}