{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n"
      ],
      "metadata": {
        "id": "hA77oQKuBzlW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzhF2lY1BbgE",
        "outputId": "7e67c05a-d7aa-409d-e22c-edb09db9c03a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'capital', 'of', 'China', 'is', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "sentence = \"The capital of China is Beijing\"\n",
        "sentence.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Issues"
      ],
      "metadata": {
        "id": "vvOhB0iSB_3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"China's capital is Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUxg2-WaBzMx",
        "outputId": "fdd43291-fdf3-4ec0-be51-0deeaca0ee18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"China's\", 'capital', 'is', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Beijing is where we'll go\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRXZxFFQCC4r",
        "outputId": "8d6c0056-1418-4ddf-a170-84a9e178c995"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'where', \"we'll\", 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I'm going to travel to Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bun372-8CGY9",
        "outputId": "0eaa73d5-b307-4062-dea1-75842782140d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'going', 'to', 'travel', 'to', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Let's travel to Hong Kong from Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtJphJsjCJGI",
        "outputId": "f6dcdc6a-7341-4f02-9416-b0b0182ad222"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A friend is pursuing his M.S from Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew_tza2uCLtv",
        "outputId": "d9f2f7cc-368a-4c55-e03e-176e0ff8b035"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types"
      ],
      "metadata": {
        "id": "8I0nddBuCWhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Regular expression based tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgeigu6vCb3Z",
        "outputId": "56a28254-4baf-40a1-8ab9-b5149c4061de"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$3000.0',\n",
              " '-',\n",
              " '$8000.0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Treebank Tokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bynn2N_CfaU",
        "outputId": "351ba289-473c-4f44-bf96-d543d2844e7b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'that',\n",
              " 'does',\n",
              " \"n't\",\n",
              " 'cost',\n",
              " 'more',\n",
              " 'than',\n",
              " '$',\n",
              " '3000.0']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tweet Tokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGesmNS7Cr8S",
        "outputId": "dbf26e19-aa10-40e1-8609-3ffd9f972f9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@amankedia',\n",
              " \"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxxxxxxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hlfeTkIC1Av",
        "outputId": "2eb94436-67c9-460b-8859-2f5151653dbc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Word Normalization"
      ],
      "metadata": {
        "id": "4myzJmL1DB7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stemming"
      ],
      "metadata": {
        "id": "XJa0qDAEDK4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "print(SnowballStemmer.languages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOVfxmYPDHlv",
        "outputId": "b0a7d916-8539-490a-9985-4e401a5f146e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned','humbled', 'sized', 'meeting', 'stating','siezing', 'itemization',\n",
        "           'traditional', 'reference', 'colonizer', 'plotted','having', 'generously']\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2phC698CDQQn",
        "outputId": "5ed7cc68-153d-4775-ed4a-87c0652c3859"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "singles = [stemmer2.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTwrV6yJDYVg",
        "outputId": "3f8b79c3-4f43-43fe-c0d3-a6a67ede3d2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization"
      ],
      "metadata": {
        "id": "Jtl2aO5ODmPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordNet Lemmatizer"
      ],
      "metadata": {
        "id": "ZRHGsbwMDu2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-HkQikgDryW",
        "outputId": "64109a21-1e74-431a-acb3-fb5bff30eea3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "s = \"We are putting in efforts to enhance our understanding of \\\n",
        "Lemmatization\"\n",
        "token_list = s.split()\n",
        "print(\"The tokens are: \", token_list)\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token \\\n",
        "in token_list])\n",
        "print(\"The lemmatized output is: \", lemmatized_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQc1evwkDzwW",
        "outputId": "23b7905d-849e-4d8f-ab15-a72e59dbc312"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
            "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DU1K-joD2Fu",
        "outputId": "1292957c-465c-4d6c-fa13-af8b5ba3f180"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = nltk.pos_tag(token_list)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukonSiGzD4nQ",
        "outputId": "12542cfd-90c2-49f8-a4b2-7860cd49d851"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('putting', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('efforts', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('enhance', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('understanding', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Lemmatization', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
        "def get_part_of_speech_tags(token):\n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focusing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "    \"N\": wordnet.NOUN,\n",
        "    \"V\": wordnet.VERB,\n",
        "    \"R\": wordnet.ADV}\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "nl0xKWIQD7PL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
        "\n",
        "print(' '.join(lemmatized_output_with_POS_information))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGX7OfGRD8wk",
        "outputId": "2cca8550-2823-4c71-d19c-18e153d2701a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We be put in effort to enhance our understand of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparision with Snowball Stemmer\n",
        "stemmer2 = SnowballStemmer(language='english')\n",
        "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
        "print(' '.join(stemmed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbKMvgIPEE02",
        "outputId": "5e9feccc-b463-425d-d50d-7326e38392a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are put in effort to enhanc our understand of lemmat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy lemmatizer"
      ],
      "metadata": {
        "id": "8qU-mM05ENsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "CUR--_jUELQP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
        "\n",
        "\" \".join([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2U8txyM1EUol",
        "outputId": "4dbde7f6-cd01-45fd-b447-3f98a18b6069"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we be put in effort to enhance our understanding of lemmatization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stopword Removal"
      ],
      "metadata": {
        "id": "3wH7Ts44EYqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHiBJ0s3EaBc",
        "outputId": "b648bc10-88eb-4e7a-b58b-e6c3b6db4416"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "\", \".join(stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "lNdw13WIEb7u",
        "outputId": "4290a82c-473e-4c91-9261-1178ed2c5cd2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"been, haven, each, their, as, will, mightn, that'll, it, don, does, o, what, shouldn, most, into, wouldn, didn't, she, is, you'll, he, such, below, had, under, ll, mightn't, just, mustn't, are, couldn't, won, how, during, weren't, this, shouldn't, down, herself, being, why, haven't, other, over, shan, our, than, do, don't, needn, were, with, until, some, didn, who, hasn, when, hasn't, those, where, ours, himself, mustn, itself, or, now, own, yours, them, wasn, aren, through, no, an, won't, you'd, up, off, y, because, m, wasn't, you're, hadn, ain, needn't, shan't, she's, if, between, from, it's, did, which, has, but, at, before, should've, yourselves, about, that, only, you, his, hadn't, while, out, both, once, after, isn't, myself, a, ma, was, here, and, in, on, aren't, ourselves, should, not, having, for, doesn't, then, theirs, s, him, of, me, have, doing, more, be, my, further, i, so, doesn, few, against, all, by, nor, same, themselves, these, any, hers, yourself, there, whom, your, very, the, too, we, again, her, wouldn't, am, re, ve, t, weren, to, can, you've, isn, its, above, they, couldn, d\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "stop = set(stopwords.words('english'))\n",
        "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "for word in wh_words:\n",
        "    stop.remove(word)\n",
        "    sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
        "\n",
        "\" \".join(sentence_after_stopword_removal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "dbtIJoWVEenj",
        "outputId": "a9f2b42c-c7ab-4b57-992c-399183669ba6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how putting efforts enhance understanding Lemmatization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Folding"
      ],
      "metadata": {
        "id": "8Sa6JPHYElfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "s = s.lower()\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s1xGSbyCEoas",
        "outputId": "ca0c23d4-4489-4011-fc66-387c36cca4ae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we are putting in efforts to enhance our understanding of lemmatization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams"
      ],
      "metadata": {
        "id": "UHVuSo65Ercj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "\n",
        "[\" \".join(token) for token in bigrams]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeBpDcp_EtbA",
        "outputId": "5c5f1465-e760-4f08-ff64-114d525aaf2b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language',\n",
              " 'Language Processing',\n",
              " 'Processing is',\n",
              " 'is the',\n",
              " 'the way',\n",
              " 'way to',\n",
              " 'to go']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "[\" \".join(token) for token in trigrams]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BzYOLw-Exl9",
        "outputId": "746a99e8-e4f3-476f-a7f2-3b02ad852694"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language Processing',\n",
              " 'Language Processing is',\n",
              " 'Processing is the',\n",
              " 'is the way',\n",
              " 'the way to',\n",
              " 'way to go']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taking care of HTML tags"
      ],
      "metadata": {
        "id": "TzIpt3U5E1OR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html)\n",
        "text = soup.get_text()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVV04-5IE2CC",
        "outputId": "db709c01-d9e2-4dae-9f95-29f2a93f6d8e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My First HeadingMy first paragraph.\n"
          ]
        }
      ]
    }
  ]
}